{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Packages ###\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Auto MPG](https://archive.ics.uci.edu/dataset/9/auto+mpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Response: mpg (continuous)\n",
    "\n",
    "Covariates: displacement, Y, cylinders, horsepower, weight, acceleration, model_year, origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/10/2lfzfs1j0j933_mjkrskp5kh0000gq/T/ipykernel_18335/3039323554.py:5: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  Auto = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/Auto.data\", delim_whitespace= True, header=None)\n"
     ]
    }
   ],
   "source": [
    "### Set up ###\n",
    "labels = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "### Import data ###\n",
    "Auto = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/Auto.data\", delim_whitespace= True, header=None)\n",
    "Auto.columns = [\"Y\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin\", \"name\"]\n",
    "Auto.drop('name', axis=1, inplace=True)\n",
    "Auto = Auto[Auto[\"horsepower\"] != \"?\"]\n",
    "Auto = Auto.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discretize data ###\n",
    "labels = [1,2,3]\n",
    "Auto[\"displacement\"] = pd.qcut(Auto[\"displacement\"], len(labels), labels=labels)\n",
    "Auto[\"cylinders\"] = pd.qcut(Auto[\"cylinders\"], len(labels), labels=labels)\n",
    "Auto[\"horsepower\"] = pd.qcut(pd.to_numeric(Auto[\"horsepower\"]), len(labels), labels=labels)\n",
    "Auto[\"weight\"] = pd.qcut(Auto[\"weight\"], len(labels), labels=labels)\n",
    "Auto[\"acceleration\"] = pd.qcut(Auto[\"acceleration\"], len(labels), labels=labels)\n",
    "Auto[\"model_year\"] = pd.qcut(Auto[\"model_year\"], len(labels), labels=labels)\n",
    "Auto[\"origin\"] = pd.Categorical(Auto[\"origin\"])\n",
    "\n",
    "### One-hot encoding ###\n",
    "# categorical_columns = [\"displacement\", \"cylinders\", \"horsepower\", \"weight\", \"acceleration\", \"model_year\", \"origin\"]\n",
    "# encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "# encoded = encoder.fit_transform(Auto[categorical_columns])\n",
    "# encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "# encoded_df = pd.DataFrame(encoded, columns=encoded_columns)\n",
    "# Auto_OneHot = pd.concat(encoded_df, [Auto[\"Y\"].reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/AutoBinned.pkl', 'wb') as file:\n",
    "        pickle.dump(Auto, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Boston Housing](/Users/simondn/Documents/RashomonActiveLearning/Data/raw/BostonHousing.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import ###\n",
    "BostonHousing = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/BostonHousing.data\", header = None, sep='\\s+')\n",
    "BostonHousing.columns = [\"CRIM\", \"ZN\", \"INDUS\", \"CHAS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\", \"MEDV\"]\n",
    "\n",
    "### Clean ###\n",
    "# KeepColumns = [\"CRIM\", \"NOX\", \"RM\", \"PTRATIO\", \"LSTAT\", \"MEDV\"]\n",
    "# BostonHousing = BostonHousing[KeepColumns]\n",
    "BostonHousing.rename(columns={'MEDV': 'Y'}, inplace=True)\n",
    "\n",
    "### Discretize data ###\n",
    "labels = [1,2,3]\n",
    "BostonHousing[\"CRIM\"] = pd.qcut(BostonHousing[\"CRIM\"], len(labels), labels=labels)\n",
    "# BostonHousing[\"ZN\"] = pd.qcut(BostonHousing[\"ZN\"], len(labels), labels=labels)\n",
    "BostonHousing[\"INDUS\"] = pd.qcut(pd.to_numeric(BostonHousing[\"INDUS\"]), len(labels), labels=labels)\n",
    "# BostonHousing[\"CHAS\"] = pd.qcut(BostonHousing[\"CHAS\"], len(labels), labels=labels)\n",
    "BostonHousing[\"NOX\"] = pd.qcut(BostonHousing[\"NOX\"], len(labels), labels=labels)\n",
    "BostonHousing[\"RM\"] = pd.qcut(BostonHousing[\"RM\"], len(labels), labels=labels)\n",
    "BostonHousing[\"AGE\"] = pd.qcut(BostonHousing[\"AGE\"], len(labels), labels=labels)\n",
    "BostonHousing[\"DIS\"] = pd.qcut(BostonHousing[\"DIS\"], len(labels), labels=labels)\n",
    "BostonHousing[\"RAD\"] = pd.qcut(BostonHousing[\"RAD\"], len(labels), labels=labels)\n",
    "BostonHousing[\"TAX\"] = pd.qcut(BostonHousing[\"TAX\"], len(labels), labels=labels)\n",
    "BostonHousing[\"PTRATIO\"] = pd.qcut(BostonHousing[\"PTRATIO\"], len(labels), labels=labels)\n",
    "BostonHousing[\"B\"] = pd.qcut(BostonHousing[\"B\"], len(labels), labels=labels)\n",
    "BostonHousing[\"LSTAT\"] = pd.qcut(BostonHousing[\"LSTAT\"], len(labels), labels=labels)\n",
    "# BostonHousing[\"Y\"] = pd.qcut(BostonHousing[\"Y\"], 2, labels=[1,2])\n",
    "BostonHousing[\"Y\"] = BostonHousing[\"Y\"] >= np.quantile(BostonHousing[\"Y\"], 0.75)\n",
    "\n",
    "## One-hot encoding ###\n",
    "categorical_columns = [\"CRIM\", \"INDUS\", \"NOX\", \"RM\", \"AGE\", \"DIS\", \"RAD\", \"TAX\", \"PTRATIO\", \"B\", \"LSTAT\"]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "encoded = encoder.fit_transform(BostonHousing[categorical_columns])\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_columns)\n",
    "BostonHousing_OneHot = pd.concat([encoded_df, BostonHousing[\"Y\"].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TreeFarmsModel[0].predict(BostonHousing_OneHot.loc[:, BostonHousing_OneHot.columns != \"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from treeFarms.treefarms.model.treefarms import TREEFARMS\n",
    "TreeFarmsModel = TREEFARMS({\"regularization\": 0.01, \"rashomon_bound_multiplier\": 0.05})\n",
    "TreeFarmsModel.fit(BostonHousing_OneHot.loc[:, BostonHousing_OneHot.columns != \"Y\"], BostonHousing_OneHot[\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TreeFarmsModel[0].predict(BostonHousing_OneHot.loc[:, BostonHousing_OneHot.columns != \"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [COMPAS](https://github.com/ubc-systopia/treeFarms/tree/main/experiments/datasets/compas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex:Female                  int64\n",
       "age:<21                     int64\n",
       "age:<23                     int64\n",
       "age:<26                     int64\n",
       "age:<46                     int64\n",
       "juvenile-felonies:=0        int64\n",
       "juvenile-misdemeanors:=0    int64\n",
       "juvenile-crimes:=0          int64\n",
       "priors:=0                   int64\n",
       "priors:=1                   int64\n",
       "priors:2-3                  int64\n",
       "priors:>3                   int64\n",
       "Y                           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMPAS = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/treeFarms/experiments/datasets/compas/binned.csv\")\n",
    "COMPAS.rename(columns={'recidivate-within-two-years:1': 'Y'}, inplace=True)\n",
    "\n",
    "### Save ###\n",
    "# with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/COMPAS.pkl', 'wb') as file:\n",
    "    # pickle.dump(COMPAS, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [MONK](https://github.com/ubc-systopia/treeFarms/tree/main/experiments/datasets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in columns ###\n",
    "MONK1 = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/MONK1.csv\",delimiter=\",\", header = 0)\n",
    "MONK2 = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/MONK2.csv\",delimiter=\",\", header = 0)\n",
    "MONK3 = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/MONK3.csv\",delimiter=\",\", header = 0)\n",
    "\n",
    "### Rename columns ###\n",
    "MONK1.rename(columns={'class_1': 'Y'}, inplace=True)\n",
    "MONK2.rename(columns={'class_1': 'Y'}, inplace=True)\n",
    "MONK3.rename(columns={'class_1': 'Y'}, inplace=True)\n",
    "\n",
    "# ### Change to categorical ###\n",
    "# MONK1 = MONK1.astype('bool')\n",
    "# MONK2 = MONK2.astype('bool')\n",
    "# MONK3 = MONK3.astype('bool')\n",
    "\n",
    "### Move columns ###\n",
    "MONK1 = MONK1.reindex(columns=['Y', 'a1_1', 'a1_2', 'a2_1', 'a2_2', 'a3_1', 'a4_1', 'a4_2', 'a5_1', 'a5_2', 'a5_3', 'a6_1',])\n",
    "MONK2 = MONK2.reindex(columns=['Y', 'a1_1', 'a1_2', 'a2_1', 'a2_2', 'a3_1', 'a4_1', 'a4_2', 'a5_1', 'a5_2', 'a5_3', 'a6_1',])\n",
    "MONK3 = MONK3.reindex(columns=['Y', 'a1_1', 'a1_2', 'a2_1', 'a2_2', 'a3_1', 'a4_1', 'a4_2', 'a5_1', 'a5_2', 'a5_3', 'a6_1',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save files ###\n",
    "with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK1.pkl', 'wb') as file:\n",
    "        pickle.dump(MONK1, file)\n",
    "\n",
    "with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK2.pkl', 'wb') as file:\n",
    "        pickle.dump(MONK2, file)\n",
    "\n",
    "with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK3.pkl', 'wb') as file:\n",
    "        pickle.dump(MONK3, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK1.pkl', 'rb') as file:\n",
    "#     test1 = pickle.load(file).dropna()\n",
    "\n",
    "# with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK2.pkl', 'rb') as file:\n",
    "#     test2 = pickle.load(file).dropna()\n",
    "\n",
    "# with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/MONK3.pkl', 'rb') as file:\n",
    "#     test3 = pickle.load(file).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load Iris Data ###\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "### Covariates ###\n",
    "X = pd.DataFrame(iris['data'])\n",
    "X.columns = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\n",
    "\n",
    "### Response ###\n",
    "y = pd.DataFrame(iris[\"target\"])\n",
    "y.columns = [\"Y\"]\n",
    "y['Y'] = y['Y'].astype(\"category\")\n",
    "\n",
    "### Discretize data ###\n",
    "labels = [1,2,3]\n",
    "X[\"SepalLength\"] = pd.qcut(X[\"SepalLength\"], len(labels), labels=labels)\n",
    "X[\"SepalWidth\"] = pd.qcut(X[\"SepalWidth\"], len(labels), labels=labels)\n",
    "X[\"PetalLength\"] = pd.qcut(X[\"PetalLength\"], len(labels), labels=labels)\n",
    "X[\"PetalWidth\"] = pd.qcut(X[\"PetalWidth\"], len(labels), labels=labels)\n",
    "\n",
    "### One-hot encoding ###\n",
    "categorical_columns = [\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "encoded = encoder.fit_transform(X[categorical_columns])\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_columns)\n",
    "Iris_OneHot = pd.concat([encoded_df, y[\"Y\"].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/Iris.pkl', 'wb') as file:\n",
    "#         pickle.dump(Iris_OneHot, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Heart Disease](https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset?resource=download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "HeartDisease = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/heart.csv\")\n",
    "HeartDisease.rename(columns={'target': 'Y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Discretize data ###\n",
    "labels = [1,2,3]\n",
    "HeartDisease[\"age\"] = pd.qcut(HeartDisease[\"age\"], len(labels), labels=labels)\n",
    "HeartDisease[\"sex\"] = pd.Categorical(HeartDisease[\"sex\"])\n",
    "HeartDisease[\"cp\"] = pd.Categorical(HeartDisease[\"cp\"])\n",
    "HeartDisease[\"trestbps\"] = pd.qcut(HeartDisease[\"trestbps\"], len(labels), labels=labels)\n",
    "HeartDisease[\"chol\"] = pd.qcut(HeartDisease[\"chol\"], len(labels), labels=labels)\n",
    "HeartDisease[\"fbs\"] = pd.Categorical(HeartDisease[\"fbs\"])\n",
    "HeartDisease[\"restecg\"] = pd.Categorical(HeartDisease[\"restecg\"])\n",
    "HeartDisease[\"thalach\"] = pd.qcut(HeartDisease[\"thalach\"], len(labels), labels=labels)\n",
    "HeartDisease[\"exang\"] = pd.Categorical(HeartDisease[\"exang\"])\n",
    "HeartDisease[\"oldpeak\"] = pd.qcut(HeartDisease[\"oldpeak\"], len(labels), labels=labels)\n",
    "HeartDisease[\"slope\"] = pd.Categorical(HeartDisease[\"slope\"])\n",
    "HeartDisease[\"ca\"] = pd.Categorical(HeartDisease[\"ca\"])\n",
    "HeartDisease[\"thal\"] = pd.Categorical(HeartDisease[\"thal\"])\n",
    "HeartDisease[\"Y\"] = pd.Categorical(HeartDisease[\"Y\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot encoding ###\n",
    "categorical_columns = [\"age\", \"sex\", \"cp\", \"trestbps\", \"chol\", \"fbs\", \"restecg\", \"thalach\", \"exang\", \"oldpeak\", \"slope\", \"ca\", \"thal\"]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "encoded = encoder.fit_transform(HeartDisease[categorical_columns])\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_columns)\n",
    "HeartDisease_OneHot = pd.concat([encoded_df, HeartDisease[\"Y\"].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# from treeFarms.treefarms.model.treefarms import TREEFARMS\n",
    "# TreeFarmsModel = TREEFARMS({\"regularization\": 0.01, \"rashomon_bound_multiplier\": 0.05})\n",
    "# TreeFarmsModel.fit(HeartDisease_OneHot.loc[:, HeartDisease_OneHot.columns != \"Y\"], HeartDisease_OneHot[\"Y\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Pima Indians Diabetes](https://www.kaggle.com/datasets/uciml/pima-indians-diabetes-database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data ###\n",
    "PimaIndians = pd.read_csv(\"/Users/simondn/Documents/RashomonActiveLearning/Data/raw/PimaIndiansDiabetes.csv\")\n",
    "PimaIndians.rename(columns={'Outcome': 'Y'}, inplace=True)\n",
    "\n",
    "### Discretize data ###\n",
    "labels = [1,2,3]\n",
    "PimaIndians[\"Pregnancies\"] = pd.qcut(PimaIndians[\"Pregnancies\"], len(labels), labels=labels)\n",
    "PimaIndians[\"Glucose\"] = pd.qcut(PimaIndians[\"Glucose\"], len(labels), labels=labels)\n",
    "PimaIndians[\"BloodPressure\"] = pd.qcut(PimaIndians[\"BloodPressure\"], len(labels), labels=labels)\n",
    "PimaIndians[\"SkinThickness\"] = pd.qcut(PimaIndians[\"SkinThickness\"], len(labels), labels=labels)\n",
    "PimaIndians[\"Insulin\"] = (PimaIndians[\"Insulin\"] == 0)\n",
    "PimaIndians[\"Insulin\"] = PimaIndians[\"Insulin\"].astype(int)\n",
    "PimaIndians[\"Insulin\"] = pd.Categorical(PimaIndians[\"Insulin\"])\n",
    "PimaIndians[\"DiabetesPedigreeFunction\"] = pd.qcut(PimaIndians[\"DiabetesPedigreeFunction\"], len(labels), labels=labels)\n",
    "PimaIndians[\"Age\"] = pd.qcut(PimaIndians[\"Age\"], len(labels), labels=labels)\n",
    "PimaIndians[\"Y\"] = pd.Categorical(PimaIndians[\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One-hot encoding ###\n",
    "categorical_columns = [\"Pregnancies\", \"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"DiabetesPedigreeFunction\", \"Age\",]\n",
    "encoder = OneHotEncoder(sparse_output=False, drop=None) \n",
    "encoded = encoder.fit_transform(PimaIndians[categorical_columns])\n",
    "encoded_columns = encoder.get_feature_names_out(categorical_columns)\n",
    "encoded_df = pd.DataFrame(encoded, columns=encoded_columns)\n",
    "PimaIndians_OneHot = pd.concat([encoded_df, PimaIndians[\"Y\"].reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/simondn/Documents/RashomonActiveLearning/Data/processed/PimaIndians.pkl', 'wb') as file:\n",
    "        pickle.dump(PimaIndians_OneHot, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\n",
      "Finding Optimal Objective...\n",
      "treefarms reported successful execution\n",
      "training completed. Number of trees in the Rashomon set: 14\n",
      "{\n",
      "  \"false\": {\n",
      "    \"complexity\": 0.009999999776482582,\n",
      "    \"loss\": 0.1419270932674408,\n",
      "    \"name\": \"Y\",\n",
      "    \"prediction\": 0\n",
      "  },\n",
      "  \"feature\": 5,\n",
      "  \"model_objective\": 0.28171876072883606,\n",
      "  \"name\": \"Glucose_3\",\n",
      "  \"reference\": 1.0,\n",
      "  \"relation\": \"==\",\n",
      "  \"true\": {\n",
      "    \"complexity\": 0.009999999776482582,\n",
      "    \"loss\": 0.1197916716337204,\n",
      "    \"name\": \"Y\",\n",
      "    \"prediction\": 1\n",
      "  },\n",
      "  \"type\": \"rational\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<treeFarms.treefarms.model.treefarms.TREEFARMS at 0x11c2fba60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from treeFarms.treefarms.model.treefarms import TREEFARMS\n",
    "TreeFarmsModel = TREEFARMS({\"regularization\": 0.01, \"rashomon_bound_multiplier\": 0.05})\n",
    "TreeFarmsModel.fit(PimaIndians_OneHot.loc[:, PimaIndians_OneHot.columns != \"Y\"], PimaIndians_OneHot[\"Y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rashomon-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
